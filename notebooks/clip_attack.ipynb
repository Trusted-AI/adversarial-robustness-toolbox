{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e111634-8795-4707-b71e-9eb2df0eba78",
   "metadata": {},
   "source": [
    "# Attacking CLIP for image classification\n",
    "\n",
    "In this notebook we show how to use the experimental tools in ART to attack the CLIP model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da58be9-e228-4928-9f73-d146cbb3cc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giulio.zizzo1/art_clip_17/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from art.estimators.hf_mm import HFMMPyTorch, MultiModalHuggingFaceInput\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "\n",
    "\n",
    "MEAN = np.asarray([0.48145466, 0.4578275, 0.40821073])\n",
    "STD = np.asarray([0.26862954, 0.26130258, 0.27577711])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9fedf31-54cf-4615-aa30-c731f1b6ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    We get sample data from the coco dataset.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import requests\n",
    "    \n",
    "    image_list = ['000000039769.jpg',\n",
    "                  '000000000285.jpg',\n",
    "                  '000000002006.jpg',\n",
    "                  '000000002149.jpg']\n",
    "\n",
    "    # Freetext description of the content of the classes we will try and sort the pictures into.\n",
    "    text = [\"a photo of a cat\", \"a photo of a bear\", \"a photo of a car\", \"a photo of a bus\", \"apples\"]\n",
    "\n",
    "    # Ground truth labels mapping the images into one of the free-text categories. \n",
    "    # Note, we do not have an image of a car in this sample of data\n",
    "    labels = torch.tensor(np.asarray([0, 1, 3, 4]))\n",
    "\n",
    "    input_list = []\n",
    "    for fname in image_list:\n",
    "        url = 'http://images.cocodataset.org/val2017/' + fname\n",
    "        input_list.append(Image.open(requests.get(url, stream=True).raw))\n",
    "\n",
    "    return input_list, text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c361f642-e60f-4aa1-8368-5b320fbc432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list, text, labels = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa53f63-eae8-44f0-ad97-df5d67d6119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_bound_eps(eps_bound=None):\n",
    "    \"\"\"\n",
    "    Helper function to normalise the l_infinity bounds from 0 - 1 into z normalization.\n",
    "    \"\"\"\n",
    "    if eps_bound is None:\n",
    "        eps_bound = np.asarray([8 / 255, 8 / 255, 8 / 255])\n",
    "    eps_bound = np.abs(eps_bound / STD)\n",
    "    return eps_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557624e0-3446-4f74-bc8f-383783e49c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(input_list, text, labels):\n",
    "    \"\"\"\n",
    "    We now attack the clip model by perturbing the input images using ARTs tools.\n",
    "    \"\"\"\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    inputs = processor(text=text, images=input_list, return_tensors=\"pt\", padding=True)\n",
    "    original_images = []\n",
    "    for i in range(3):\n",
    "        original_images.append(inputs[\"pixel_values\"][i].clone().cpu().detach().numpy())\n",
    "    original_images = np.concatenate(original_images)\n",
    "\n",
    "    art_classifier = HFMMPyTorch(\n",
    "        model, \n",
    "        loss=loss_fn,\n",
    "        nb_classes=5,\n",
    "        clip_values=(np.min(original_images), np.max(original_images)), \n",
    "        input_shape=(3, 224, 224)\n",
    "    )\n",
    "\n",
    "    art_input = MultiModalHuggingFaceInput(**inputs)\n",
    "    clean_preds = art_classifier.predict(art_input)\n",
    "    clean_acc = np.sum(np.argmax(clean_preds, axis=1) == labels.cpu().detach().numpy()) / len(labels)\n",
    "    print('The clean accuracy is ', clean_acc)\n",
    "\n",
    "    attack = ProjectedGradientDescent(\n",
    "        art_classifier,\n",
    "        max_iter=10,\n",
    "        eps=np.ones((3, 224, 224)) * np.reshape(norm_bound_eps(), (3, 1, 1)),\n",
    "        eps_step=np.ones((3, 224, 224)) * 0.1,\n",
    "    )\n",
    "    x_adv = attack.generate(art_input, labels)\n",
    "    adv_preds = art_classifier.predict(x_adv)\n",
    "    adv_acc = np.sum(np.argmax(adv_preds, axis=1) == labels.cpu().detach().numpy()) / len(labels)\n",
    "\n",
    "    print('The adversarial accuracy is ', clean_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f011a60-2381-4d3f-866a-a39ae2279dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 06:11:36.199655: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-17 06:11:36.232269: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-17 06:11:36.232299: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-17 06:11:36.232327: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-17 06:11:36.240857: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-17 06:11:37.143845: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The clean accuracy is  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PGD - Iterations: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.61it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 39.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adversarial accuracy is  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Running the attack we see the performance drop from 100% to 0%.\n",
    "attack(input_list, text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332f36d-531c-4311-b6e2-32bb22d9f835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
