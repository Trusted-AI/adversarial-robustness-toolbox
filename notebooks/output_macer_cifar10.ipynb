{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "output_macer_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> MACER Algorithm to train provably robust models </h1>\n",
        "This notebook demonstrates how to use the ART library to learn robust model on CIFAR-10 dataset using MACER algorithm. <br>\n",
        "In this example notebook we will be showing MACER algorithm implementation using PyTorch.\n",
        "\n"
      ],
      "metadata": {
        "id": "tSUPU4QbvNIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's walk through some initial work steps ensuring that the notebook will work smoothly. We will:\n",
        "\n",
        "1. set up a small configuration cell\n",
        "2. load data and apply transformations on the data\n",
        "3. define and load the model (resnet110)\n",
        "4. define the optimizer (SGD) and the schedular (MultiStep)\n",
        " "
      ],
      "metadata": {
        "id": "fpziawDHxwuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from art.utils import load_dataset, random_targets, compute_accuracy,load_cifar10\n",
        "from art.estimators.certification.randomized_smoothing import (PyTorchRandomizedSmoothing)\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from art.estimators.classification.pytorch import PyTorchClassifier \n",
        "from art.data_generators import PyTorchDataGenerator"
      ],
      "metadata": {
        "id": "_MRo_HrVvSKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    print(\"Using CUDA\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "4n6ENG-GvhZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Load Data </h1>\n",
        "We are loading CIFAR10 dataset and applying transformations (random cropping, random horizontal flip, convert image to Tensor) "
      ],
      "metadata": {
        "id": "2F0LFK1AvuuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_data = datasets.CIFAR10(\"./dataset_cache\", train=True, download=True, transform=transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor()\n",
        "        ]))\n",
        "test_data = datasets.CIFAR10(\"./dataset_cache\", train=False, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=1)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size,\n",
        "                             num_workers=1, pin_memory=True)"
      ],
      "metadata": {
        "id": "ZqMOJMw2v6OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_samples = 50000\n",
        "\n",
        "x_train = torch.zeros((num_train_samples, 3, 32, 32), dtype=torch.float32)\n",
        "y_train = torch.zeros((num_train_samples,), dtype=torch.uint8)\n",
        "\n",
        "for i,(data,labels) in enumerate(train_loader):\n",
        "    x_train[(i) * batch_size : (i+1) * batch_size, :, :, :] = data\n",
        "    y_train[(i) * batch_size : (i+1) * batch_size] = labels"
      ],
      "metadata": {
        "id": "S_opPHKLv6KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_samples = 10000\n",
        "\n",
        "x_test = torch.zeros((num_train_samples, 3, 32, 32), dtype=torch.float32)\n",
        "y_test = torch.zeros((num_train_samples,), dtype=torch.uint8)\n",
        "\n",
        "for i,(data,labels) in enumerate(test_loader):\n",
        "    x_test[(i) * batch_size : (i+1) * batch_size, :, :, :] = data\n",
        "    y_test[(i) * batch_size : (i+1) * batch_size] = labels"
      ],
      "metadata": {
        "id": "X0bwykCmv6Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Train Classifiers </h1>\n",
        "1. Defining and loading the resnet110 model.<br>\n",
        "2. Defining the optimizer and scheduler"
      ],
      "metadata": {
        "id": "AohDGtZcwa6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "  \" 3x3 convolution with padding \"\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      residual = self.downsample(x)\n",
        "\n",
        "    out += residual\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet_Cifar(nn.Module):\n",
        "\n",
        "  def __init__(self, block, layers, width=1, num_classes=10):\n",
        "    super(ResNet_Cifar, self).__init__()\n",
        "    self.inplanes = 16\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
        "                           stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.layer1 = self._make_layer(block, 16 * width, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 32 * width, layers[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 64 * width, layers[2], stride=2)\n",
        "    self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "    self.fc = nn.Linear(64 * block.expansion * width, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "    downsample = None\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "      downsample = nn.Sequential(\n",
        "          nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                    kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(planes * block.expansion)\n",
        "      )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "    self.inplanes = planes * block.expansion\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet110(**kwargs):\n",
        "  model = ResNet_Cifar(BasicBlock, [18, 18, 18], width=1, **kwargs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "wfICsrJXvnE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet110()"
      ],
      "metadata": {
        "id": "DwRCss13ww5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = MultiStepLR(optimizer, milestones=[200,400], gamma=0.1)"
      ],
      "metadata": {
        "id": "Va-UNd6hwxUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "NadJUTy-8Znl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>We are now ready to employ the ART library and train a provably robust smoothed classifier using MACER algorithm.</h3>\n",
        "We will use PyTorchRandomizedSmoothing class of ART library to define the classifier and finally, fit the classifier using macer train_method. "
      ],
      "metadata": {
        "id": "y6M46SJe3Wbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigma_1 = 0.25\n",
        "rs_macer_classifier = PyTorchRandomizedSmoothing(model=model,\n",
        "    optimizer=optimizer,\n",
        "    input_shape=(3, 32, 32),\n",
        "    nb_classes=10,\n",
        "    scale=sigma_1,\n",
        "    lbd = 12.0,\n",
        "    gamma = 8.0,\n",
        "    beta = 16.0,\n",
        "    gauss_num = 16,\n",
        "    scheduler = scheduler)"
      ],
      "metadata": {
        "id": "0hd7YMN-w3a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_macer_classifier.fit(x_train, y_train, nb_epochs=440, batch_size=64, train_method = 'macer')"
      ],
      "metadata": {
        "id": "e-SCC-2hxB1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can start training at a particular checkpoint by passing the path to checkpoint in the fit function argument.<br>\n",
        "<br>\n",
        "For Example,\n",
        "<br>\n",
        "rs_macer_classifier.fit(x_train, y_train, nb_epochs=440, batch_size=64, train_method = 'macer', checkpoint = path_to_checkpoint)"
      ],
      "metadata": {
        "id": "0pzQIg1K5j9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Predictions </h1>\n",
        "Now we will use the trained provably robust smoothed classifier to predict the test dataset."
      ],
      "metadata": {
        "id": "e1UZhhKyxWPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We one hot encode the output of test data, predict the first 500 images of the test data, and compute the accuracy and coverage."
      ],
      "metadata": {
        "id": "X5DRWjdV8gHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_encoded = F.one_hot(y_test.to(torch.int64))"
      ],
      "metadata": {
        "id": "u6kGDD2667zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_preds_rs_1 = rs_macer_classifier.predict(x_test[:500])\n",
        "acc_rs_1, cov_rs_1 = compute_accuracy(x_preds_rs_1, y_test_encoded[:500].numpy())\n",
        "print(\"\\nSmoothed Classifier, sigma=\" + str(sigma_1))\n",
        "print(\"Accuracy: {}\".format(acc_rs_1))\n",
        "print(\"Coverage: {}\".format(cov_rs_1))"
      ],
      "metadata": {
        "id": "WhHEDhe53jf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Certification </h1>\n",
        "We will now certify our classifier to prove that our trained model can achieve provable robustness against any possible attack in the certified region."
      ],
      "metadata": {
        "id": "gVRX7yoD-jQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define some helpful Python functions for certification"
      ],
      "metadata": {
        "id": "BzlLnl0w9-iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate certification accuracy for a given radius\n",
        "def getCertAcc(radius, pred, y_test):\n",
        "\n",
        "    rad_list = np.linspace(0, 2.25, 201)\n",
        "    cert_acc = []\n",
        "    num_cert = len(radius)\n",
        "    \n",
        "    for r in rad_list:\n",
        "        rad_idx = np.where(radius >= r)[0]\n",
        "        y_test_subset = y_test[rad_idx]\n",
        "        cert_acc.append(np.sum(pred[rad_idx] == y_test_subset) / num_cert)\n",
        "    return cert_acc"
      ],
      "metadata": {
        "id": "ZVUsnO8ixCiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateACR(target, prediction, radius):\n",
        "  tot = 0\n",
        "  cnt = 0\n",
        "  for i in range(0,len(prediction)):\n",
        "    #class_index = np.where(target[i] == 1.0)\n",
        "    if(prediction[i] == target[i]):\n",
        "      tot += radius[i]\n",
        "    cnt += 1\n",
        "  return tot/cnt"
      ],
      "metadata": {
        "id": "9APEIt6wyY08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Certified Radius for single image </h1>"
      ],
      "metadata": {
        "id": "0yDFxi9C4Wr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#single image certification return certified radius, index or random \n",
        "index = random.randint(0,9999)\n",
        "x_sample = x_test[index].expand((1,3,32,32))\n",
        "prediction, radius = rs_macer_classifier.certify(x_sample, n = 100000)\n",
        "print(\"Prediction: {} and Radius: {}\".format(prediction,radius))"
      ],
      "metadata": {
        "id": "yE8Zd49e4a9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Certification on test images</h3>"
      ],
      "metadata": {
        "id": "G23-ed0u8ziP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_img = 500\n",
        "num_img = 500\n",
        "skip = 1\n",
        "N = 100000"
      ],
      "metadata": {
        "id": "-U9xuWN39AJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#no.of test images for ACR/graph (ACR inside the graph)\n",
        "prediction_1, radius_1 = rs_macer_classifier.certify(x_test[(start_img-1):(start_img-1)+(num_img*skip):skip], n=N)"
      ],
      "metadata": {
        "id": "oOeewhC9ybFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acr = calculateACR(target=np.array(y_test[(start_img-1):(start_img-1)+(num_img*skip):skip]), prediction= np.array(prediction_1), radius = np.array(radius_1))\n",
        "print(\"ACR: \",acr)"
      ],
      "metadata": {
        "id": "kNOSo0naPhCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rad_list = np.linspace(0, 2.25, 201)\n",
        "plt.plot(rad_list, getCertAcc(radius_1, prediction_1, np.array(y_test)), 'r-', label='smoothed, $\\sigma=$' + str(sigma_1))\n",
        "plt.xlabel('l2 radius')\n",
        "plt.ylabel('certified accuracy')\n",
        "plt.legend()\n",
        "plt.title('Radius Accuracy Curves: ACR {}'.format(acr))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WdkIt5pBzIo6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}