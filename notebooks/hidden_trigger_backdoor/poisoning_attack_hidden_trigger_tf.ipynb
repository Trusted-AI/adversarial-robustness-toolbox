{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Trigger Backdoor Attack Example\n",
    "This notebook shows how to use the ART implementation of the Hidden Trigger Backdoor attack to poison a model. The goal of the attack is to create poisoned inputs within a target class with no visible trigger that can be used during model finetuning to poison the model. After finetuning, adding a trigger to certain inputs not in the target class will cause the model to reliably misclassify those inputs as the target class. Full details of the attack can be found at https://arxiv.org/abs/1910.00033."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary directory not created yet\n",
      "Temporary directory: /tmp/tmpkokqkf5u\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from os.path import abspath\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tempfile\n",
    "# Create a temporary directory for the model checkpoint. Remove the exising one if this cell is rerun\n",
    "try:\n",
    "    temp_model_dir.cleanup()\n",
    "except NameError:\n",
    "    print(\"Temporary directory not created yet\")\n",
    "finally:\n",
    "    temp_model_dir = tempfile.TemporaryDirectory() \n",
    "    print(\"Temporary directory:\", temp_model_dir.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Model\n",
    "We will load the CIFAR10 dataset and a pre-trained alexnet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.utils import load_dataset\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset('cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 20:42:09.241104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-07 20:42:09.252305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-07 20:42:09.253212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-07 20:42:09.254690: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-07 20:42:09.255814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-07 20:42:09.256647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-07 20:42:09.257480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-07 20:42:09.903621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-07 20:42:09.904467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-07 20:42:09.905355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-07 20:42:09.906125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory:  -> device: 0, name: NVIDIA Tesla P100-PCIE-16GB, pci bus id: 0000:00:06.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models, losses\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Source here: https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
    "model.add(layers.Conv2D(32, (3, 3), padding=\"same\", activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def train_step(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "loss_object = losses.CategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "\n",
    "classifier = TensorFlowV2Classifier(\n",
    "    model=model,\n",
    "    loss_object=loss_object,\n",
    "    train_step=train_step,\n",
    "    nb_classes=10,\n",
    "    input_shape=(32, 32, 3),\n",
    "    clip_values=(min_, max_),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 20:42:14.500283: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2022-02-07 20:42:14.865093: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 20:43:31.210986: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkokqkf5u/htbd_model/assets\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(x_train, y_train, nb_epochs=10, batch_size=128)\n",
    "model.save(temp_model_dir.name + '/htbd_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 76.22%\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(x_test)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Poison\n",
    "We now will generate the poison using the hidden trigger backdoor attack. First, we define the target and source classes as well as the backdoor trigger. The target class will be the class we want to insert poisoned data into. The source class will be the class we will add a trigger to in order to cause misclassification into the target.\n",
    "\n",
    "The backdoor trigger will be a small image patch inserted into the source class images. At test time, we should be able to use this trigger to cause the classifier to misclassify source class images with the trigger added as the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.poisoning.backdoor_attack import PoisoningAttackBackdoor\n",
    "target = np.array([0,0,0,0,1,0,0,0,0,0])\n",
    "source = np.array([0,0,0,1,0,0,0,0,0,0])\n",
    "\n",
    "# Backdoor Trigger Parameters\n",
    "patch_size = 8\n",
    "x_shift = 32 - patch_size - 5\n",
    "y_shift = 32 - patch_size - 5\n",
    "\n",
    "# Define the backdoor poisoning object. Calling backdoor.poison(x) will insert the trigger into x.\n",
    "from art.attacks.poisoning import perturbations\n",
    "def mod(x):\n",
    "    original_dtype = x.dtype\n",
    "    x = perturbations.insert_image(x, backdoor_path=\"../../utils/data/backdoors/htbd.png\",\n",
    "                                   channels_first=False, random=False, x_shift=x_shift, y_shift=y_shift,\n",
    "                                   size=(patch_size,patch_size), mode='RGB', blend=1)\n",
    "    return x.astype(original_dtype)\n",
    "backdoor = PoisoningAttackBackdoor(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the attack. `eps` controls how much the target images can be perturbed with respect to an l-infinity distance. `feature_layer` dicates with layer's output will be used to define the attack's loss. It can either be the name of the layer or the layer index according to the ART estimator. `poison_percent` controls how many poisoned samples will be generated based on the size of the input data.\n",
    "\n",
    "The attack will return poisoned inputs of the target class and the indicies in the data that those poisoned inputs should replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1406adaa95437b9b269c1e06bd83ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hidden Trigger:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | batch: 0 | i:     0 | LR: 0.01000 | Loss Val: 2852.665 | Loss Avg: 2852.665\n",
      "Epoch:  0 | batch: 0 | i:   100 | LR: 0.01000 | Loss Val: 272.750 | Loss Avg: 396.859\n",
      "Epoch:  0 | batch: 0 | i:   200 | LR: 0.01000 | Loss Val: 242.497 | Loss Avg: 325.773\n",
      "Epoch:  0 | batch: 0 | i:   300 | LR: 0.01000 | Loss Val: 240.480 | Loss Avg: 297.527\n",
      "Epoch:  0 | batch: 0 | i:   400 | LR: 0.01000 | Loss Val: 231.204 | Loss Avg: 281.951\n",
      "Epoch:  0 | batch: 0 | i:   500 | LR: 0.01000 | Loss Val: 228.130 | Loss Avg: 272.251\n",
      "Epoch:  0 | batch: 0 | i:   600 | LR: 0.01000 | Loss Val: 227.712 | Loss Avg: 265.148\n",
      "Epoch:  0 | batch: 0 | i:   700 | LR: 0.01000 | Loss Val: 224.862 | Loss Avg: 259.692\n",
      "Epoch:  0 | batch: 0 | i:   800 | LR: 0.01000 | Loss Val: 218.741 | Loss Avg: 255.377\n",
      "Epoch:  0 | batch: 0 | i:   900 | LR: 0.01000 | Loss Val: 212.148 | Loss Avg: 250.801\n",
      "Epoch:  0 | batch: 0 | i:  1000 | LR: 0.00100 | Loss Val: 205.692 | Loss Avg: 246.497\n",
      "Epoch:  0 | batch: 0 | i:  1100 | LR: 0.00100 | Loss Val: 199.862 | Loss Avg: 242.303\n",
      "Epoch:  0 | batch: 0 | i:  1200 | LR: 0.00100 | Loss Val: 199.485 | Loss Avg: 238.747\n",
      "Epoch:  0 | batch: 0 | i:  1300 | LR: 0.00100 | Loss Val: 199.215 | Loss Avg: 235.714\n",
      "Epoch:  0 | batch: 0 | i:  1400 | LR: 0.00100 | Loss Val: 198.974 | Loss Avg: 233.100\n",
      "Epoch:  0 | batch: 0 | i:  1500 | LR: 0.00100 | Loss Val: 198.919 | Loss Avg: 230.825\n",
      "Epoch:  0 | batch: 0 | i:  1600 | LR: 0.00100 | Loss Val: 198.789 | Loss Avg: 228.827\n",
      "Epoch:  0 | batch: 0 | i:  1700 | LR: 0.00100 | Loss Val: 198.689 | Loss Avg: 227.058\n",
      "Epoch:  0 | batch: 0 | i:  1800 | LR: 0.00100 | Loss Val: 198.650 | Loss Avg: 225.481\n",
      "Epoch:  0 | batch: 0 | i:  1900 | LR: 0.00100 | Loss Val: 198.452 | Loss Avg: 224.065\n",
      "Epoch:  0 | batch: 0 | i:  2000 | LR: 0.00010 | Loss Val: 198.546 | Loss Avg: 222.787\n",
      "Epoch:  0 | batch: 0 | i:  2100 | LR: 0.00010 | Loss Val: 198.362 | Loss Avg: 221.625\n",
      "Epoch:  0 | batch: 0 | i:  2200 | LR: 0.00010 | Loss Val: 198.355 | Loss Avg: 220.568\n",
      "Epoch:  0 | batch: 0 | i:  2300 | LR: 0.00010 | Loss Val: 198.347 | Loss Avg: 219.602\n",
      "Epoch:  0 | batch: 0 | i:  2400 | LR: 0.00010 | Loss Val: 198.337 | Loss Avg: 218.716\n",
      "Epoch:  0 | batch: 0 | i:  2500 | LR: 0.00010 | Loss Val: 198.332 | Loss Avg: 217.901\n",
      "Epoch:  0 | batch: 0 | i:  2600 | LR: 0.00010 | Loss Val: 198.323 | Loss Avg: 217.149\n",
      "Epoch:  0 | batch: 0 | i:  2700 | LR: 0.00010 | Loss Val: 198.319 | Loss Avg: 216.452\n",
      "Epoch:  0 | batch: 0 | i:  2800 | LR: 0.00010 | Loss Val: 198.315 | Loss Avg: 215.804\n",
      "Epoch:  0 | batch: 0 | i:  2900 | LR: 0.00010 | Loss Val: 198.308 | Loss Avg: 215.201\n",
      "Max_Loss: 198.3076599075148\n",
      "Epoch:  0 | batch: 1 | i:     0 | LR: 0.01000 | Loss Val: 2484.048 | Loss Avg: 215.400\n",
      "Epoch:  0 | batch: 1 | i:   100 | LR: 0.01000 | Loss Val: 243.875 | Loss Avg: 219.997\n",
      "Epoch:  0 | batch: 1 | i:   200 | LR: 0.01000 | Loss Val: 224.665 | Loss Avg: 220.558\n",
      "Epoch:  0 | batch: 1 | i:   300 | LR: 0.01000 | Loss Val: 218.629 | Loss Avg: 220.793\n",
      "Epoch:  0 | batch: 1 | i:   400 | LR: 0.01000 | Loss Val: 219.105 | Loss Avg: 220.821\n",
      "Epoch:  0 | batch: 1 | i:   500 | LR: 0.01000 | Loss Val: 210.612 | Loss Avg: 220.745\n",
      "Epoch:  0 | batch: 1 | i:   600 | LR: 0.01000 | Loss Val: 218.155 | Loss Avg: 220.683\n",
      "Epoch:  0 | batch: 1 | i:   700 | LR: 0.01000 | Loss Val: 211.011 | Loss Avg: 220.504\n",
      "Epoch:  0 | batch: 1 | i:   800 | LR: 0.01000 | Loss Val: 220.103 | Loss Avg: 220.354\n",
      "Epoch:  0 | batch: 1 | i:   900 | LR: 0.01000 | Loss Val: 205.309 | Loss Avg: 220.176\n",
      "Epoch:  0 | batch: 1 | i:  1000 | LR: 0.00100 | Loss Val: 210.104 | Loss Avg: 219.963\n",
      "Epoch:  0 | batch: 1 | i:  1100 | LR: 0.00100 | Loss Val: 199.685 | Loss Avg: 219.487\n",
      "Epoch:  0 | batch: 1 | i:  1200 | LR: 0.00100 | Loss Val: 199.219 | Loss Avg: 219.010\n",
      "Epoch:  0 | batch: 1 | i:  1300 | LR: 0.00100 | Loss Val: 199.007 | Loss Avg: 218.548\n",
      "Epoch:  0 | batch: 1 | i:  1400 | LR: 0.00100 | Loss Val: 198.822 | Loss Avg: 218.102\n",
      "Epoch:  0 | batch: 1 | i:  1500 | LR: 0.00100 | Loss Val: 198.654 | Loss Avg: 217.671\n",
      "Epoch:  0 | batch: 1 | i:  1600 | LR: 0.00100 | Loss Val: 198.497 | Loss Avg: 217.257\n",
      "Epoch:  0 | batch: 1 | i:  1700 | LR: 0.00100 | Loss Val: 198.513 | Loss Avg: 216.857\n",
      "Epoch:  0 | batch: 1 | i:  1800 | LR: 0.00100 | Loss Val: 198.295 | Loss Avg: 216.472\n",
      "Epoch:  0 | batch: 1 | i:  1900 | LR: 0.00100 | Loss Val: 198.227 | Loss Avg: 216.101\n",
      "Epoch:  0 | batch: 1 | i:  2000 | LR: 0.00010 | Loss Val: 198.091 | Loss Avg: 215.743\n",
      "Epoch:  0 | batch: 1 | i:  2100 | LR: 0.00010 | Loss Val: 198.039 | Loss Avg: 215.396\n",
      "Epoch:  0 | batch: 1 | i:  2200 | LR: 0.00010 | Loss Val: 198.040 | Loss Avg: 215.062\n",
      "Epoch:  0 | batch: 1 | i:  2300 | LR: 0.00010 | Loss Val: 198.026 | Loss Avg: 214.741\n",
      "Epoch:  0 | batch: 1 | i:  2400 | LR: 0.00010 | Loss Val: 198.018 | Loss Avg: 214.431\n",
      "Epoch:  0 | batch: 1 | i:  2500 | LR: 0.00010 | Loss Val: 198.016 | Loss Avg: 214.133\n",
      "Epoch:  0 | batch: 1 | i:  2600 | LR: 0.00010 | Loss Val: 198.008 | Loss Avg: 213.845\n",
      "Epoch:  0 | batch: 1 | i:  2700 | LR: 0.00010 | Loss Val: 197.992 | Loss Avg: 213.567\n",
      "Epoch:  0 | batch: 1 | i:  2800 | LR: 0.00010 | Loss Val: 197.986 | Loss Avg: 213.299\n",
      "Epoch:  0 | batch: 1 | i:  2900 | LR: 0.00010 | Loss Val: 197.986 | Loss Avg: 213.039\n",
      "Max_Loss: 197.97221781588541\n",
      "Epoch:  0 | batch: 2 | i:     0 | LR: 0.01000 | Loss Val: 2827.333 | Loss Avg: 213.226\n",
      "Epoch:  0 | batch: 2 | i:   100 | LR: 0.01000 | Loss Val: 324.456 | Loss Avg: 217.019\n",
      "Epoch:  0 | batch: 2 | i:   200 | LR: 0.01000 | Loss Val: 280.096 | Loss Avg: 218.207\n",
      "Epoch:  0 | batch: 2 | i:   300 | LR: 0.01000 | Loss Val: 255.315 | Loss Avg: 218.909\n",
      "Epoch:  0 | batch: 2 | i:   400 | LR: 0.01000 | Loss Val: 245.673 | Loss Avg: 219.406\n",
      "Epoch:  0 | batch: 2 | i:   500 | LR: 0.01000 | Loss Val: 256.729 | Loss Avg: 219.819\n",
      "Epoch:  0 | batch: 2 | i:   600 | LR: 0.01000 | Loss Val: 237.385 | Loss Avg: 220.140\n",
      "Epoch:  0 | batch: 2 | i:   700 | LR: 0.01000 | Loss Val: 246.310 | Loss Avg: 220.457\n",
      "Epoch:  0 | batch: 2 | i:   800 | LR: 0.01000 | Loss Val: 240.189 | Loss Avg: 220.685\n",
      "Epoch:  0 | batch: 2 | i:   900 | LR: 0.01000 | Loss Val: 235.135 | Loss Avg: 220.876\n",
      "Epoch:  0 | batch: 2 | i:  1000 | LR: 0.00100 | Loss Val: 231.568 | Loss Avg: 221.075\n",
      "Epoch:  0 | batch: 2 | i:  1100 | LR: 0.00100 | Loss Val: 223.070 | Loss Avg: 221.115\n",
      "Epoch:  0 | batch: 2 | i:  1200 | LR: 0.00100 | Loss Val: 222.403 | Loss Avg: 221.137\n",
      "Epoch:  0 | batch: 2 | i:  1300 | LR: 0.00100 | Loss Val: 222.045 | Loss Avg: 221.151\n",
      "Epoch:  0 | batch: 2 | i:  1400 | LR: 0.00100 | Loss Val: 221.835 | Loss Avg: 221.162\n",
      "Epoch:  0 | batch: 2 | i:  1500 | LR: 0.00100 | Loss Val: 221.612 | Loss Avg: 221.168\n",
      "Epoch:  0 | batch: 2 | i:  1600 | LR: 0.00100 | Loss Val: 221.281 | Loss Avg: 221.172\n",
      "Epoch:  0 | batch: 2 | i:  1700 | LR: 0.00100 | Loss Val: 221.202 | Loss Avg: 221.174\n",
      "Epoch:  0 | batch: 2 | i:  1800 | LR: 0.00100 | Loss Val: 221.156 | Loss Avg: 221.174\n",
      "Epoch:  0 | batch: 2 | i:  1900 | LR: 0.00100 | Loss Val: 221.013 | Loss Avg: 221.173\n",
      "Epoch:  0 | batch: 2 | i:  2000 | LR: 0.00010 | Loss Val: 220.895 | Loss Avg: 221.170\n",
      "Epoch:  0 | batch: 2 | i:  2100 | LR: 0.00010 | Loss Val: 220.732 | Loss Avg: 221.165\n",
      "Epoch:  0 | batch: 2 | i:  2200 | LR: 0.00010 | Loss Val: 220.713 | Loss Avg: 221.159\n",
      "Epoch:  0 | batch: 2 | i:  2300 | LR: 0.00010 | Loss Val: 220.705 | Loss Avg: 221.154\n",
      "Epoch:  0 | batch: 2 | i:  2400 | LR: 0.00010 | Loss Val: 220.696 | Loss Avg: 221.149\n",
      "Epoch:  0 | batch: 2 | i:  2500 | LR: 0.00010 | Loss Val: 220.694 | Loss Avg: 221.143\n",
      "Epoch:  0 | batch: 2 | i:  2600 | LR: 0.00010 | Loss Val: 220.684 | Loss Avg: 221.138\n",
      "Epoch:  0 | batch: 2 | i:  2700 | LR: 0.00010 | Loss Val: 220.676 | Loss Avg: 221.133\n",
      "Epoch:  0 | batch: 2 | i:  2800 | LR: 0.00010 | Loss Val: 220.668 | Loss Avg: 221.128\n",
      "Epoch:  0 | batch: 2 | i:  2900 | LR: 0.00010 | Loss Val: 220.662 | Loss Avg: 221.122\n",
      "Max_Loss: 220.6338108681748\n",
      "Number of poison samples generated: 75\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.poisoning import HiddenTriggerBackdoor\n",
    "poison_attack = HiddenTriggerBackdoor(classifier, eps=16/255, target=target, source=source, feature_layer=9, backdoor=backdoor, learning_rate=0.01, decay_coeff = .1, decay_iter = 1000, max_iter=3000, batch_size=25, poison_percent=.015)\n",
    "\n",
    "poison_data, poison_indices = poison_attack.poison(x_train, y_train)\n",
    "print(\"Number of poison samples generated:\", len(poison_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the Model\n",
    "Now, we must finetune the model using the poisoned data and a small number of clean training inputs.  Here, we randomly select an equal number of training inputs from each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create finetuning dataset\n",
    "dataset_size = 2500\n",
    "num_classes = 10\n",
    "num_per_class = dataset_size/num_classes\n",
    "\n",
    "poison_dataset_inds = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    class_inds = np.where(np.argmax(y_train,axis=1) == i)[0]\n",
    "    num_select = int(num_per_class)\n",
    "    if np.argmax(target) == i:\n",
    "        num_select = int(num_select - min(num_per_class,len(poison_data)))\n",
    "        poison_dataset_inds.append(poison_indices)\n",
    "        \n",
    "    if num_select != 0:\n",
    "        poison_dataset_inds.append(np.random.choice(class_inds, num_select, replace=False))\n",
    "    \n",
    "poison_dataset_inds = np.concatenate(poison_dataset_inds)\n",
    "\n",
    "poison_x = np.copy(x_train)\n",
    "poison_x[poison_indices] = poison_data\n",
    "poison_x = poison_x[poison_dataset_inds]\n",
    "\n",
    "poison_y = np.copy(y_train)[poison_dataset_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,250,858\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(temp_model_dir.name + '/htbd_model')\n",
    "temp_model_dir.cleanup() # Remove the temporary directory after loading the checkpoint\n",
    "\n",
    "model.trainable = False\n",
    "model.compile(loss=losses.CategoricalCrossentropy(from_logits=True), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 76.22%\n"
     ]
    }
   ],
   "source": [
    "classifier = TensorFlowV2Classifier(\n",
    "    model=model,\n",
    "    nb_classes=10,\n",
    "    input_shape=(32, 32, 3),\n",
    "    clip_values=(min_, max_),\n",
    ")\n",
    "predictions = classifier.predict(x_test)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18003\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_input (InputLayer)    [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 1,245,728\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "finetune_model = tf.keras.layers.Dense(10)(model.layers[-2].output)\n",
    "finetune_model = tf.keras.Model(inputs=model.inputs, outputs=finetune_model)\n",
    "finetune_model.summary()\n",
    "\n",
    "lr = 0.5\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "finetune_classifier = TensorFlowV2Classifier(\n",
    "    model=finetune_model,\n",
    "    loss_object=loss_object,\n",
    "    train_step=train_step,\n",
    "    nb_classes=10,\n",
    "    input_shape=(32, 32, 3),\n",
    "    clip_values=(min_, max_),\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 10.75%\n",
      "Accuracy on benign trigger test examples: 22.6%\n",
      "Accuracy on poison trigger test examples: 79.4%\n",
      "Success on poison trigger test examples: 7.6%\n",
      "\n",
      "Training Epoch 0\n",
      "Accuracy on benign test examples: 72.78999999999999%\n",
      "Accuracy on benign trigger test examples: 37.6%\n",
      "Accuracy on poison trigger test examples: 5.4%\n",
      "Success on poison trigger test examples: 54.300000000000004%\n",
      "\n",
      "Training Epoch 1\n",
      "Accuracy on benign test examples: 74.59%\n",
      "Accuracy on benign trigger test examples: 62.5%\n",
      "Accuracy on poison trigger test examples: 14.299999999999999%\n",
      "Success on poison trigger test examples: 56.10000000000001%\n",
      "\n",
      "Training Epoch 2\n",
      "Accuracy on benign test examples: 74.77000000000001%\n",
      "Accuracy on benign trigger test examples: 58.9%\n",
      "Accuracy on poison trigger test examples: 12.8%\n",
      "Success on poison trigger test examples: 55.400000000000006%\n",
      "\n",
      "Training Epoch 3\n"
     ]
    }
   ],
   "source": [
    "trigger_test_inds = np.where(np.all(y_test == source, axis=1))[0]\n",
    "\n",
    "test_poisoned_samples, test_poisoned_labels  = backdoor.poison(x_test[trigger_test_inds], y_test[trigger_test_inds])\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    predictions = finetune_classifier.predict(x_test)\n",
    "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "    print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
    "    \n",
    "    predictions = finetune_classifier.predict(x_test[trigger_test_inds])\n",
    "    b_accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[trigger_test_inds], axis=1)) / len(trigger_test_inds)\n",
    "    print(\"Accuracy on benign trigger test examples: {}%\".format(b_accuracy * 100))\n",
    "    \n",
    "    predictions = finetune_classifier.predict(test_poisoned_samples)\n",
    "    p_accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(test_poisoned_labels,axis=1)) / len(test_poisoned_labels)\n",
    "    print(\"Accuracy on poison trigger test examples: {}%\".format(p_accuracy * 100))\n",
    "    p_success = np.sum(np.argmax(predictions, axis=1) == np.argmax(target)) / len(test_poisoned_labels)\n",
    "    print(\"Success on poison trigger test examples: {}%\".format(p_success * 100))\n",
    "    print()\n",
    "    print(\"Training Epoch\", i)\n",
    "    if i != 0:\n",
    "        lr *= 0.1\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    finetune_classifier = TensorFlowV2Classifier(\n",
    "        model=finetune_model,\n",
    "        loss_object=loss_object,\n",
    "        train_step=train_step,\n",
    "        nb_classes=10,\n",
    "        input_shape=(32, 32, 3),\n",
    "        clip_values=(min_, max_),\n",
    "    )\n",
    "\n",
    "    finetune_classifier.fit(poison_x, poison_y, nb_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Performance\n",
      "Accuracy on benign test examples: 74.86%\n",
      "Accuracy on benign trigger test examples: 58.9%\n",
      "Accuracy on poison trigger test examples: 12.5%\n",
      "Success on poison trigger test examples: 55.800000000000004%\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Performance\")\n",
    "predictions = finetune_classifier.predict(x_test)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "predictions = finetune_classifier.predict(x_test[trigger_test_inds])\n",
    "b_accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[trigger_test_inds], axis=1)) / len(trigger_test_inds)\n",
    "print(\"Accuracy on benign trigger test examples: {}%\".format(b_accuracy * 100))\n",
    "\n",
    "predictions = finetune_classifier.predict(test_poisoned_samples)\n",
    "p_accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(test_poisoned_labels,axis=1)) / len(test_poisoned_labels)\n",
    "print(\"Accuracy on poison trigger test examples: {}%\".format(p_accuracy * 100))\n",
    "p_success = np.sum(np.argmax(predictions, axis=1) == np.argmax(target)) / len(test_poisoned_labels)\n",
    "print(\"Success on poison trigger test examples: {}%\".format(p_success * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
