{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSUPU4QbvNIq"
      },
      "source": [
        "<h3> ART SMOOTHADVERSARIAL </h3>\n",
        "\n",
        "Provably robust deep learning via adversarially trained smoothed classifier.\n",
        "\n",
        "Link to the paper: [https://arxiv.org/abs/1906.04584](https://arxiv.org/abs/1906.04584)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_MRo_HrVvSKH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "\n",
        "from art.utils import load_dataset, random_targets, compute_accuracy,load_cifar10\n",
        "from art.estimators.certification.randomized_smoothing import PyTorchRandomizedSmoothing\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from art.estimators.classification.pytorch import PyTorchClassifier \n",
        "from art.data_generators import PyTorchDataGenerator\n",
        "\n",
        "import math\n",
        "from art.estimators.classification.pytorch import PyTorchClassifier \n",
        "from art.data_generators import PyTorchDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n6ENG-GvhZ1"
      },
      "outputs": [],
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    print(\"Using CUDA\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F0LFK1AvuuB"
      },
      "source": [
        "<h3> Load Data </h3>\n",
        "\n",
        "Loading the data to be used to train the model on. Using the CIFAR10 dataset here with data augmentation and preprocessing applied.\n",
        "\n",
        "Training data: 50000\n",
        "Test data: 10000\n",
        "Input Shape: (3,32,32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqMOJMw2v6OO"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_data = datasets.CIFAR10(\"./dataset_cache\", train=True, download=True, transform=transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor()\n",
        "        ]))\n",
        "test_data = datasets.CIFAR10(\"./dataset_cache\", train=False, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=1)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size,\n",
        "                             num_workers=1, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S_opPHKLv6KJ"
      },
      "outputs": [],
      "source": [
        "num_train_samples = 50000\n",
        "\n",
        "x_train = torch.zeros((num_train_samples, 3, 32, 32), dtype=torch.float32)\n",
        "y_train = torch.zeros((num_train_samples,), dtype=torch.uint8)\n",
        "\n",
        "for i,(data,labels) in enumerate(train_loader):\n",
        "    x_train[(i) * batch_size : (i+1) * batch_size, :, :, :] = data\n",
        "    y_train[(i) * batch_size : (i+1) * batch_size] = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X0bwykCmv6Gz"
      },
      "outputs": [],
      "source": [
        "num_train_samples = 10000\n",
        "\n",
        "x_test = torch.zeros((num_train_samples, 3, 32, 32), dtype=torch.float32)\n",
        "y_test = torch.zeros((num_train_samples,), dtype=torch.uint8)\n",
        "\n",
        "for i,(data,labels) in enumerate(test_loader):\n",
        "    x_test[(i) * batch_size : (i+1) * batch_size, :, :, :] = data\n",
        "    y_test[(i) * batch_size : (i+1) * batch_size] = labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AohDGtZcwa6I"
      },
      "source": [
        "<h3> Train SmoothAdversarial Classifier </h3>\n",
        "\n",
        "Training the smooth adversarial classifier. Building the model first using the ResNet110 architecture. Then the optimizer and scheduler functions are defined along with the Randomized Smoothing class object. The fit invocation trains the model on the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_CIFAR10_MEAN = [0.4914, 0.4822, 0.4465]\n",
        "_CIFAR10_STDDEV = [0.2023, 0.1994, 0.2010]\n",
        "class NormalizeLayer(torch.nn.Module):\n",
        "    \"\"\"Standardize the channels of a batch of images by subtracting the dataset mean\n",
        "      and dividing by the dataset standard deviation.\n",
        "      In order to certify radii in original coordinates rather than standardized coordinates, we\n",
        "      add the Gaussian noise _before_ standardizing, which is why we have standardization be the first\n",
        "      layer of the classifier rather than as a part of preprocessing as is typical.\n",
        "      \"\"\"\n",
        "\n",
        "    def __init__(self, means, sds):\n",
        "        \"\"\"\n",
        "        :param means: the channel means\n",
        "        :param sds: the channel standard deviations\n",
        "        \"\"\"\n",
        "        super(NormalizeLayer, self).__init__()\n",
        "        self.means = torch.tensor(means).cuda()\n",
        "        self.sds = torch.tensor(sds).cuda()\n",
        "\n",
        "    def forward(self, input: torch.tensor):\n",
        "        (batch_size, num_channels, height, width) = input.shape\n",
        "        means = self.means.repeat((batch_size, height, width, 1)).permute(0, 3, 1, 2)\n",
        "        sds = self.sds.repeat((batch_size, height, width, 1)).permute(0, 3, 1, 2)\n",
        "        return (input - means) / sds\n",
        "def get_num_classes(dataset: str):\n",
        "    \"\"\"Return the number of classes in the dataset. \"\"\"\n",
        "    if dataset == \"imagenet\":\n",
        "        return 1000\n",
        "    elif dataset == \"cifar10\":\n",
        "        return 10\n",
        "\n",
        "\n",
        "def get_normalize_layer(dataset: str) -> torch.nn.Module:\n",
        "    \"\"\"Return the dataset's normalization layer\"\"\"\n",
        "    if dataset == \"cifar10\":\n",
        "        return NormalizeLayer(_CIFAR10_MEAN, _CIFAR10_STDDEV)"
      ],
      "metadata": {
        "id": "9jU7f2ibXKrw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "wfICsrJXvnE3"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "  \" 3x3 convolution with padding \"\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      residual = self.downsample(x)\n",
        "\n",
        "    out += residual\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet_Cifar(nn.Module):\n",
        "\n",
        "  def __init__(self, block, layers, width=1, num_classes=10):\n",
        "    super(ResNet_Cifar, self).__init__()\n",
        "    self.inplanes = 16\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
        "                           stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.layer1 = self._make_layer(block, 16 * width, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 32 * width, layers[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 64 * width, layers[2], stride=2)\n",
        "    self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "    self.fc = nn.Linear(64 * block.expansion * width, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "    downsample = None\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "      downsample = nn.Sequential(\n",
        "          nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                    kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(planes * block.expansion)\n",
        "      )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "    self.inplanes = planes * block.expansion\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet110(**kwargs):\n",
        "  model = ResNet_Cifar(BasicBlock, [18, 18, 18], width=1, **kwargs)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "DwRCss13ww5S"
      },
      "outputs": [],
      "source": [
        "model = resnet110()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_layer = get_normalize_layer(\"cifar10\")\n",
        "model = torch.nn.Sequential(normalize_layer, model)"
      ],
      "metadata": {
        "id": "NifnxDiGsDCN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Va-UNd6hwxUp"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "loss = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0hd7YMN-w3a_"
      },
      "outputs": [],
      "source": [
        "sigma_1 = 0.25 #Noise\n",
        "rs_smoothadv_classifier = PyTorchRandomizedSmoothing(model=model,\n",
        "                          clip_values=(0.0, 255.0),\n",
        "                          optimizer=optimizer,\n",
        "                          scheduler = scheduler,\n",
        "                          loss = loss,\n",
        "                          input_shape=(3, 32, 32),\n",
        "                          nb_classes=10,\n",
        "                          scale=sigma_1, \n",
        "                          num_noise_vec= 8,\n",
        "                          train_multi_noise= True,\n",
        "                          attack_type=\"PGD\",\n",
        "                          epsilon=1.0,\n",
        "                          num_steps=10,\n",
        "                          warmup=10\n",
        "                          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rs_smoothadv_classifier.fit(x_train, y_train, nb_epochs=150, batch_size=256, train_method = 'smoothadv')"
      ],
      "metadata": {
        "id": "LsNw9dH6sNb2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1UZhhKyxWPJ"
      },
      "source": [
        "<h3> Predictions for Trained Model </h3>\n",
        "\n",
        "Predicting on the test dataset using the trained model. The accuracy and coverage for the trained model is observed. Coverage refers to the percentage number of instances that could be classified and the model did not abstain from providing a predicted label."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_encoded = F.one_hot(y_test.to(torch.int64))"
      ],
      "metadata": {
        "id": "DtFT7NkDtnBw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_preds_rs_1 = rs_smoothadv_classifier.predict(x_test[:500])\n",
        "acc_rs_1, cov_rs_1 = compute_accuracy(x_preds_rs_1, y_test_encoded[:500].numpy())\n",
        "print(\"\\nSmoothedAdversarial Classifier, sigma=\" + str(sigma_1))\n",
        "print(\"Accuracy: {}\".format(acc_rs_1))\n",
        "print(\"Coverage: {}\".format(cov_rs_1))"
      ],
      "metadata": {
        "id": "1vDoGRFjtr82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZVUsnO8ixCiL"
      },
      "outputs": [],
      "source": [
        "# Calculate certification accuracy for a given radius\n",
        "def getCertAcc(radius, pred, y_test):\n",
        "    rad_list = np.linspace(0, 2.25, 201)\n",
        "    cert_acc = []\n",
        "    num_cert = len(radius)\n",
        "    for r in rad_list:\n",
        "        rad_idx = np.where(radius >= r)[0]\n",
        "        y_test_subset = y_test[rad_idx]\n",
        "        cert_acc.append(np.sum(pred[rad_idx] == y_test_subset) / num_cert)\n",
        "    return cert_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9APEIt6wyY08"
      },
      "outputs": [],
      "source": [
        "def calculateACR(target, prediction, radius):\n",
        "  tot = 0\n",
        "  cnt = 0\n",
        "  for i in range(0,len(prediction)):\n",
        "      if(prediction[i] == target[i]):\n",
        "          tot += radius[i]\n",
        "      cnt += 1\n",
        "  return tot/cnt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Certified Radius for Single Image</h3>\n",
        "\n",
        "Calculating the certified radius on a single image. The image index to be used is used randomly from the 10000 test images. To use any particular image, use the corresponding image's index below."
      ],
      "metadata": {
        "id": "E7UNR33uuJRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#single image certification return certified radius, index or random) \n",
        "index = random.randint(0,9999)\n",
        "x_sample = x_test[index].expand((1,3,32,32))\n",
        "prediction, radius = rs_smoothadv_classifier.certify(x_sample, n = 100000)\n",
        "print(\"Prediction: {} and Radius: {}\".format(prediction,radius))"
      ],
      "metadata": {
        "id": "mIL_8mdguQB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Certification on Test Images</h3>\n",
        "\n",
        "Performing and observing the certification over all the test dataset consisting of 10000 images. The ACR (Average Certified Radius) is computed for the certification results to understand results better."
      ],
      "metadata": {
        "id": "aPC4-rVDuYsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no.of test images for ACR/graph (ACR inside the graph)\n",
        "start_img = 500\n",
        "num_img = 500\n",
        "skip = 1\n",
        "N = 100000"
      ],
      "metadata": {
        "id": "nIO05s7ruejE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_1, radius_1 = rs_smoothadv_classifier.certify(x_test[(start_img-1):(start_img-1)+(num_img*skip):skip], n=N)"
      ],
      "metadata": {
        "id": "vxeDLsQ_uj5E"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acr = calculateACR(target=np.array(y_test[(start_img-1):(start_img-1)+(num_img*skip):skip]), prediction= np.array(prediction_1), radius = np.array(radius_1))\n",
        "print(\"ACR for Smooth Adversarial Classifier: \", acr)"
      ],
      "metadata": {
        "id": "4uB-JdsPuyQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rad_list = np.linspace(0, 2.25, 201)\n",
        "plt.plot(rad_list, getCertAcc(radius_1, prediction_1, np.array(y_test)), 'r-', label='smoothed, $\\sigma=$' + str(sigma_1))\n",
        "plt.xlabel('l2 radius')\n",
        "plt.ylabel('Certified Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Average Certified Radius plot: ACR {}'.format(acr))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k1wyS05Mu7WH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "output_smoothadv_cifar10_pytorch_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}